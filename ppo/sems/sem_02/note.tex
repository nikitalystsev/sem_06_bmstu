\documentclass[12pt]{article} % Устанавливаем базовый размер шрифта 12 пунктов

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}

% Настройка полей
\geometry{
	a4paper,
	left=2cm,
	right=2cm,
	top=2cm,
	bottom=2cm,
}

\begin{document}


%*00:00:00*
Пересечь туда, куда будет удобно.  Смотрите, о чем сегодня будем говорить.  Сегодня будем говорить о самом верхнем уровне корректирования и архитектуры программных систем.  Подвинутый уровень. Мы с вами пойдем не по тому принципу, который заложен в лекции по умолчанию. В лекциях вам рассказывают сначала про то, как правильно классы писать, потом про то, как правильно классы в компоненты объединять, потом про то, как компоненты правильно друг к другу соединять, потом про то, как из компонентов получается большая система и может быть в конце чуть-чуть расскажет про то, как же вся эта штука выглядит на верхнем уровне когда вы там много приложений, как эти приложения друг к другу взаимодействуют где-то там пользуясь интерфейсом, как это все вместе связано Я решил, что в продвинутом курсе мы пойдем наоборот Мы сначала поговорим про то, что называется System Design, то есть верхний уровень И сегодня не будет никаких подробностей про то, как правильно написать класс, как правильно написать компонент. Будет только про то, как, собственно, в верхней схеме, потому что я тогда 80\% современной заработал, пожалуй, устроена на верхнем уровне вся эта история.  потом мы с вами чуть-чуть погрузимся в самые важные и интересные штуки на уровне сочетания компонентов и архитектуры в рамках приложения на третьем, если я смогу это все уложить в что-то вроде лекции по одному семинару поговорим про то, что будет ниже. Мы с вами пойдем сверху вниз, а не снизу вверх потому что таким образом, когда мы будем переходить на более низкий уровень, вы уже вы будете понимать, зачем вам это нужно, а не наоборот.  Так, ну у меня есть возможность сделать там чистую архитектуру, зачем она нужна, не очень понятно.  Да, тут написано за 20 минут.  Вот не верьте, скорее всего, я даже в пару выложусь, но я постараюсь.  Да, для начала давайте немножко поговорим про то, что такое ЕF.  у вас будет отдельный курс по нему в следующем семестре, но я сейчас коротенько, за 5 минут расскажу о чем.  Собственно, есть у нас такая штука, называется Интернет, все им пользуются.  И на самом деле эта штука сильно шире, чем просто возможность посещать какие-то сайтики, это возможность создавать локальные сети, которые могут быть как изолированы от Интернета, так и подключены к нему через какие-то шлюшки.

*00:03:29*
И при появлении собственного интернета, ближайшие 2-3 семинара у нас будут больше в дикционном формате, а вот уже потом мы начнем с вами разбирать всякие кейсы, я буду давать вам всякие задания, возможно даже домашние на проработку какой-то архитектуры, а потом мы будем их разбирать.  Но это будет после того, как я вам четко скажу, чтобы вы четко понимали.  Возможно, к следующим или через одному семинару я уже буду давать задания.  Скорее всего, они будут в группах.  Их не то, чтобы безумно обязательно делать.  Если вы их не сделаете, вас никто не убьет.  Просто не сможете на семинаре что-то подробно разобрать.  Баллов за них особо не будет.  Ну, там, за Денис Евгеньевич нарисовал какие-то 5 баллов, чтобы я их мог вам выдавать.  Но ключевое это скорее там для вашего интереса и понимания.  

Да, возвращаясь к архитектуре Client-Server.  Так что сегодня, как бы, то, что я вам написал, посчитать, оно будет полезно, чтобы вы лучше понимали вот это, но не будет. Сегодня, как бы, больше лекция.  Да, Client-Server — это архитектура, когда у вас...  Архитектура на уровне, вот, собственно, когда у вас есть сеть и какие-то приложения, то есть отдельные программы, которые взаимодействуют друг с другом.  Взаимодействуют они при помощи, собственно, сети, при помощи каких-то сетевых протоколов.  Чаще всего это HTTP, про которое вы, скорее всего, слышали так или иначе.  И при этом взаимодействии у вас одна программа выступает в роли клиента, другая в роли сервера.  Что это означает? Это означает, что программа, которая выступает в роли клиента, отправляет запрос на сервер и получает на него ответ.  Самое важное. В клиент-серверной архитектуре всегда у вас есть клиенты, всегда есть сервер.  Они могут время от времени меняться местами. У вас может быть 2 программы, которые обе знают адреса друг друга и поочередно ходят друг к другу с запросами. Но вот в один момент времени тот, кто отправил запрос, он клиент.  Тот, кто отвечает на запрос, он сервер.  

Соответственно, когда мы говорим про типичный веб и какого-то пользователя, который пользуется каким-то сайтом, приложением, системой, то у нас положение клиент-сервера достаточно устаканено.  Вы, заходя на какой-нибудь сайт, и ваш браузер, либо ваше мобильное приложение, из которого вы заказываете себе такси, выступает в роли клиента, в роли сервера, соответственно, выступают какие-то там сервера и backend приложения, которые стоят на стране компании, с которой вы работаете.

%*00:05:57*
Ну, условно, заказываете вы такси, Яндекс.Такси, ваше приложение, которое вы скачали, выступает в роли клиента,  сервера Яндекса, на которых, собственно, висит всякий процессинг, вычисления алгоритмов, выдача там типа еще кому-то каких-то заказов, ну уже самим таксистом, да, выступает в роли сервера. 

Взаимодействие происходит при помощи различных сетевых протоколов, зачастую это HTTP.  Суть как подробно мы с вами разбирать его не будем, потому что нам на самом деле оно немножко бы сильно надо.  Ключевое, что там есть запрос и мы можем получить на него ответ.  

Если у вас где-то появляются вопросы, поднимайте руку по ходу дела, я потом обращу на вас внимание и отвечу на вопросы.  Идем дальше. Соответственно, погружаемся чуть-чуть глубже, потому что архитектура клиент-сервера называется архитектурой, но архитектуры там, по какому-то базовому принципу, больше ничего.  

Теперь давайте поговорим о том, как примерно устроен клиент, то есть мобильный, ну и по факту даже не мобильное приложение, а как устроен веб-сайт современный или не очень современный, какие там бывают варианты взаимодействия.  

Их два, в скобочках там, и комбинации какие-то, т.е. основных вариантов два.  Смотрите, опять же, у нас сейчас не курс по вербу, я вам не буду рассказывать, как писать на JavaScript-е приложение, или как в HTML верстать. Это будет в следующем семестре, и уже нет.  Потому что я отказался от того, чтобы сделать курс. В прошлом семестре это был последний, так что там меня уже не встретят.  

Что здесь важно понимать?  Взаимодействие зачастую идет по HTTP и там есть какие-то запросы.  Ну, собственно, здесь все запросы, ответы там стрелочками отображены между клиентом и сервером.  Есть то, что называется многостраничное приложение и символ PHP, что значит одностраничное приложение.  

Изначально весь интернет появился как многостраничное приложение.  Характернейший пример это Википедия. Вы заходите на Википедию, у вас открывается страница, вы кликаете по какой-то ссылке, отправляется новый запрос на сервер, на самом деле, в этот момент, и вы получаете полностью новую страницу. Она полностью загружается. У вас происходит перезагрузка в браузере, да, если у вас супер медленный интернет, он там даже в чуть подумает. Вот. Вы получаете новую страницу. Ну, Википедия, у вас там просто набор статичных страниц, которые вам с бекенда просто отдаются, как бы, как есть.  Там просто html, который одинаков для всех.

%*00:08:34*
html это язык разметки для, собственно, вашего браузера, по которому он умеет рендерить картинку.  Естественно, приложения бывают не только такие, в которых нужно рендерить статичный файлик.  Бывает, что нужно туда что-то подставлять.  Не пользователя, а возможные товары, его какие-то покупки, не знаю, настройки и так далее.  

В этом случае, в первой версии, собственно, ну и все это дело пошло по такому пути.  Клиент делает запрос, он в нем, возможно, в этом запросе указывает какой-то набор параметров, там, входных данных, в общем, ну, там, вместе с запросом что-то передается.  Сервер получает это, обрабатывает запрос, он, вероятно, там, откуда-то получил идентификатор пользователя, он получил еще какие-то данные, в общем, ему нужно сгенерировать какую-то страницу.  И сервер, при помощи специального шаблона, на нем программист пишет шаблон html-страниц, и в ней указывает, типа, сюда подставь username, сюда подставь, ну, я не знаю, что-то еще, сюда сгенерирую целым списком, типа, там, список товаров вот по такому шаблону товаров, который описан, собственно, на html.  

В итоге сервер генерирует этот, так называемый, сервер-сайт рендеринг, либо, собственно, шаблонизируемый рендеринг.  вы получаете какой-то html, уникальный под этот запрос, и он возвращается клиенту.  

Что важно в этом MPA?  Каждый раз, когда вы нажимаете какую-то кнопку, каждый раз, когда вы делаете какой-то запрос, происходит полный перерисовка на страницы. Backend-ом называется то, что на северной стороне, Frontend-ом то, что на клиентской стороне.  С сервера вам идет, соответственно, полная HTML.  
Это минус, потому что, во-первых, у вас постоянно идет перезагрузка страницы, во-вторых, у вас постоянно передается довольно большой поток данных по сети.  Ну, HTML, он большой.  Текстовый, там много данных, там много всяких открывающихся, он многословный.  Если там когда-нибудь случайно открывали в браузере страничку разработчиков, наверняка ужасались вот эти вот портянки, короче, всяких вот кавычек, фишей и кучи всего непонятного.  

Из плюсов.  Браузеру нужно, на самом деле, точнее, не браузеру, а поисковикам, нужно как-то индексировать сайты, чтобы вам их в поисковой выдаче выдавать.  Они это делают путем, ну, банальной, короче, поиска по всему интернету.  Они заходят к каким то ботам на сайт и его как-то размечают для себя.

%*00:11:22*

Да, это MPA у вас открываются в центральной страничке сайта, когда она открывается, у нее уже есть все данные.  Она просто как HTML с сервера отдалась, там все есть, ее очень легко разметить.  В случае с SinglePageApplication это немножко не так.  Так что в этом плане попроще.  Но еще MPA обычно быстрее можно сделать на коленке.  Потом будет сильно больнее для пользователя так себе UX, но вообще можно сделать быстро, если вам нужна какая-нибудь админка для внутреннего приложения, или клиенту надо, чтобы работало, и не говорим, вообще насрать, вот типо нормальный вариант. 

Следующий вариант, как он работает. В какой-то момент появился такой язык, как JavaScript, и это скриптовый язык, который во всех браузерах имеет возможность перехватывать, ну точнее не перехватывать, а работать с так называемым домом, то есть по факту с HTML составляющей страницы и менять ее прямо во время своего выполнения.  

И придумали следующее. На старте загружается какая-то страничка и кусок JavaScript кода, в котором записан целый фреймворк, который умеет, короче, ловко вертеть всеми html тегами, после чего уже клиент отвечает за рендеринг своей странички, а бэкэнд с клиентом обменивается только данными. То есть мы со скрипта отправляем запрос и говорим, слышь, бэкэнд, дай нам список товаров, не html всех товаров для пользователя, а просто список товаров. Получаем в ответ список товаров в каком-то из распространенных форматов. Чаще всего сейчас это JSON, раньше использовали XMA, может быть еще что-то в более хитром, бинарном формате какого-нибудь там gRPC или еще чем-нибудь.  Это сейчас вообще не важно. Важно, что вы получаете данные с бэкэнда, только данные. И клиент отвечает полностью за отрисовку.  

Мобильное приложение работает точно так же по принципу SPA, потому что там как бы одна страничка, да, вы с бэкэндом обмениваетесь только данными.  Но это преимущественного понятия MPA и SPA меняются к именно вебу, то есть не к мобильным приложениям, а к страницам интернета.  Про клиент все.  Вам на текущий момент этого, пожалуй, хватит, потом подробнее можно будет поговорить.  Если интересно, опять же, читайте, я вам базовые слова какие-то даю, чуть-чуть я про них объясняю, можете дальше погружаться.

%*00:14:00*

Сегодня будет именно такая больше обзорная история, что вы общую картинку понимали, как выглядят приложения, ну скажем так, за рамками лабораторной работы, когда вам нужно сделать, принести преподавателю какую-то лампу. 

Собственно, сейчас мы начнем погружаться в Web. Тут, видите, есть рисунок 1.1.  Это, на самом деле, глава из книжки. Я вам потом его дам. Даже, скорее всего, скинул...  очень неправильно скинул полетную книжку в чате, чтобы вы ее не искали.  Это первая глава в этой книжке.  Вот там первая глава прям хорошая, она прям объясняет, как приложение от вот такого вот простой схемки превращается в фанкенштейна, до которого мы сегодня постараемся дойти.  

Значит, что здесь у нас есть?  У нас есть какой-то пользователь, который пользуется либо браузером, либо мобильным приложением.  И есть веб-сервер.  Пока что эта схема очень похожа на то, что вы видели до этого про клиент-сверх, правильно?  Единственная новая штука, которая здесь появилась, это DNS.  

DNS это система номерных имен.  Ее суть в том, что весь интернет, точнее весь интернет текущий практически работает поверх протокола IP и поверх, ну, чаще всего TCP, ну, либо UDP.  Но UDP тоже использует IP-стек.  

Что такое IP-адрес?  IP-адрес — это адрес вашего какого-то компьютера в сети.  Сеть может быть локальная, сеть может быть глобальная.  В большинстве случаев, когда мы говорим про ваш компьютер, текущий, подключенный к интернету, на самом деле ваш адрес будет в локальной сети, а это локальная сеть через какой-то дополнительный, короче, кусочек, подключенный уже к глобальной.  

Но, возможно, кто-то из вас слышал такое понятие, как белый IP-адрес.  Белый IP-адреса — это IP-адрес, который существует в глобальной сети, в большой.  И к нему можно обратиться с любого другого, естественно, адреса в глобальной сети.  Чтобы человеку не приходилось запоминать набор из цифр, в случае с IPv6, это более новая версия IP, там вообще, как бы, они такой адрес не запомнят точно никак.  Вот, ну, как бы, гораздо приятнее переходить на я.ру или там google.com, да, чем на что-нибудь там, я не знаю, 112.14.5.8.  При этом, как бы, вообще-то, мы ходим именно по эти адресам.  DNS придуман отчасти для того, чтобы решить эту проблему.

%*00:16:40*
В нем есть еще довольно большое количество другой информации возможно мы с вами к этому еще вернемся но как бы глобально сейчас нужно понять: DNS это штука сторонняя глобальная который умеет вам отвечать. Вот этому домену соответствует вот такой IP-адрес --- иди к нему. Он на самом деле ну отчасти рекусивный корневой DNS-сервер не знает какой IP-адрес у api.mysite.com, но он знает, какой другой DNS-сервер отвечает за зону .com.  

Сервер, который отвечает за зону .com, знает, кто отвечает за mysite.com, но не знает, кто отвечает за api.mysite.com.  Понятно, да, примерно так устроено? То есть, есть корневой DNS-сервер, он знает о тех, кто отвечает за зону .com, зону RU, зону там, я не знаю, какой-нибудь Soviet Union домен, еще какие-то домены.  

Короче, они знают про то, что уже внутри них.  И дальше, иерархически, вы можете так дойти до домена какой-то компании, там уже будет DNS-сервер, который мейнтейнится этой компанией, и она уже внутри себя разруливает все, там, API mysite call, mydevsite.mysite.com и так далее.  Там вы уже можете строить все, что хотите.  

Важно, чтобы в корневом DNS-сервере была запись, которая ведет на ваш DNS-сервер либо непосредственно на ваш сайт.  И тогда все будет работать.  Опять же, интересно подробнее, задавайте вопросы в конце, либо еще где-то, либо читайте сами, как работает.  Нам сейчас важно, что DNS-сервер умеет возвращать нам по IP-адресу домен.  о, точнее, по домену IP-address.  И дальше наше приложение по этому IPзaddress умеет ходить на наш веб-сервер.  Начинаем усложнять.  

В большинстве приложений нам нужна база данных, ну, потому что мы хотим хранить какие-то данные.  Ну, добавим базу данных.  В общем-то, ничего сложного.  Зачастую используются револяционные базы данных, у вас уже был по ним курс, вы примерно представляете, что это такое.  Надеюсь.  По идее, даже курсач, то ли сейчас пишет это, то ли что-то вот такого рода.  Это может быть нереляционная база, но нам сейчас это не так важно.  Дальше начинается прикол.  

Смотрите, у нас есть один сервер, и если мы накатываем на него какое-то обновление, то он может в какое-то время быть недоступен.  А еще может получиться так, что нагрузки на этот сервер будет больше, чем может выдержать один сервер.  А еще может произойти так, что наше приложение периодически падает.  Оно как бы падает очень редко, но все же падает.  И в этот момент времени, ну, короче, хочется, чтобы тоже все было доступно.

%*00:19:20*

И на этом этапе появляется новый компонент этой системы, который называется балансировщик нагрузки.  да контрольный вопрос вы видите сейчас схему сколько запущенных приложений вы на ней видите сколько приложений написанных когда-то разработчиками вами либо другими вашей системы вот этой вашей системы либо какими-то другими сторонними разработчиками. Сколько приложений вы здесь видите запущенных?  Какие?  Нет, больше. Ну, да. То есть, минимум? Ну, на самом деле, да, уникальных получается минимум 5, потому что на сервере мы написали одно и то же приложение, спустили его в двух копиях.  чтобы у нас одинаковый сервер работал.  Но глобально тут минимум 5 приложений.  

И дальше сегодня мы с вами не будем погружаться в то, как конкретные приложения в деталях разрабатываются, мы будем говорить это квадратик, оно работает.  И выполняет свою функцию.  Бизнес-логика какая-то нашего приложения, она как-то между клиентским и бэкенд-приложением размазана.  Чаще больше в бэкенде, меньше в фронтенде.  Балансировщик отвечает только за то, чтобы входящий трафик балансировать между двумя серверами.  

Как видите, здесь у вас появились такие внутренние IP-адреса, 10 точка.  Вот внутренний IP-адрес, это адрес во внутренней сети.  Сейчас у нас сервер уже не торчит во внешнюю сеть, все запросы приходят на балансировщик нагрузки.  В балансировщик нагрузки он имеет белый IP-адрес, торчит во внешнюю сеть, и, соответственно, с ним все могут работать.  А сервера дальше стоят в внутренней сети, соединенные с балансировщиком и получают от него запросы.  

Если один из серверов перестал отвечать, балансировщик это через какое-то время поймет и перенаправится с трафиком в второй, например.  Там можно по-разному сильно настраивать балансировщик. Мы сейчас с вами об этом тоже прямо говорить детально не будем.  Может, когда-то тоже подойдем, а может, не подойдем.  Но причем, что нужно понять, что есть такая штука, как балансировщики нагрузки, она создана ровно для того, чтобы входящий трафик распределять в каком-то соотношении с какими-то правилами, сложными или не очень, между серверами.  И решает она какую проблему? Она решает проблему А. Недоступности одного из серверов.  Б. Это недостаточно. Ну, просто, когда мощности одного сервера не хватает, нам нужно больше серверов.  Так. Понятно ли, как эта схема работает?  Давайте так. На всякий случай, кому непонятно, поднимите руку.  Нормально, что вам непонятно. Ничего страшного в этом нет.  Все всё поняли? Допустим.

%*00:22:22* 

Следующая проблема, которая перед нами с вами встает, вот там веб сервисы и там вот вся вот эта предыдущая картинка и всем этим серверам нужно что-то писать в базу данных так вот, внезапно оказывается, что у нас как бы есть одна база данных, она классная, только есть нюанс поднимем RAIDист, все данные пользователей, ой, кончились, и все, что делать непонятно. Первый вариант проблемы.  

Второй вариант проблемы.  У нас огромное количество пользователей растет.  Их становится больше.  И у нас долгие операции на запись.  Ну, потому что мы там создаем, открываем транзакцию, накидываем какое-то количество сущностей в базу, закрываем транзакцию.  Она параллельно может залочить таблицу, потому что там, короче, ну, критически важно, чтобы данные были консистентными, ну вот, такого порядка штуки, они могут долго выполняться.  

И при этом у нас, на самом деле, 4, 5, 10, 20 раз больше запросов на чтение, потому что пользователи редко данные редактируют, но очень часто их читают. Ну, какая-нибудь соцсеть, типа народ пост новый пишет, сильно реже, чем не ставит, короче, ленту и читает какие-то штуки.  Но это как пример. На самом деле большинство этих приложений, это такой паттерн.  Читают больше, чем пишут. Это чаще так происходит.  Бывают виды приложений, где это не так, но в целом читают чаще, чем пишут.  

И решением двух этих вариантов проблем является репликация базы данных.  Слышали когда-нибудь, что такое?  Рассказывали вам на базе данных про репликацию или нет?  Что-то вы про нее поняли?  Звучит так, что ничего.  Можете объяснить, попробовать, что такое репликация?  Ну, если мы упрощены, да.  Смотрите, как работает репликация.  Она бывает разных типов. Бывает мастер слейд, бывает мастер мастер.  Да, отличное название, которое вы придумали еще в другом выпуске.  Так вот.  У вас всегда есть хотя бы одна ведущая база данных, которая содержит максимально консистентное, максимально актуальное состояние.  Она всегда будет называться мастер.  И дальше, ну, по умолчанию, самый простой вариант организации Организация это вот мастер слейд, то есть ведущая, ведомая база данных.

*00:25:28*
В чем суть? Довольно сложно.  Когда у вас один сервер ВВД, то очень легко сделать его консистентным.  То есть вы в него и пишете, из него и читаете, и как бы вы можете легко разгруливать то, насколько запросы, ну, данные между такими серверами одинаковые.  Вот у меня один сервер, у него данные актуальны.  Когда у вас становится четыре сервера, когда вам запрос приходит в одну базу данных, Если вы хотите, чтобы он вот прям ровно сейчас, ровно в этот момент времени оказался во всех остальных серверах, вам как бы нужно запрос по идее закончить и сказать, что он закончится только тогда, когда эти данные реально уже на все остальные сервера как-то переехали по сети.  Это долго.  Поэтому часто бывает так, что на самом деле вот в ведомом FBD данные, они как бы почти актуальны.  они, типа, туда переписываются, но, ну, может быть, с каким-то лагом.  И если для вас супер критично, то вы можете сделать этот лаг около нулевым.  Ну, или там, нулевым. Тогда это чуть-чуть красотка производительности.  Либо вы можете наоборот. Вы можете сказать, ну, мне в целом окей, что там, типа, минутка данных какая-то, ну, не... данные с минутной актуальностью отсутствуют, Оно у меня будет переписываться большими пачками раз в какое-то количество секунд.  В случае, даже если мы ведущую ВД потеряем, у меня все равно есть все данные, ну кроме там последней какой-то...  последней минуты, это не критично. Потому что мы считаем, что этот отказ супер редкий.  Короче, ну, не будем об этом говорить. Разберемся, если что.  компенсацию выплатили, еще что-то, но потеряли несколько.  Если это прям супер критикал, там транзакции по деньги, там еще что-то, там очень важно.  Из ведома, то есть тех, кто реплик, которые просто, ну из реплик по факту, да, которые читают данные и сохраняют всякую копию, а да, они сохраняют себе полную копию того, что есть в ведущей БД.  Обычно, как происходит, совершается набор транзакций, об этих транзакциях пишутся какие-то сообщения, и эти сообщения о транзакциях, об этих изменениях, они, ну, практически, как в GTA условно, там, изменения в таблице какой-то происходят.  Они, короче, транслируются в другую базу данных и применяются на ней.  А таких вот ведомых баз может быть сколько угодно, таких баз можно читать, но в них нельзя писать.  Потому что, если вы начнете писать, навременно, в две базы, то вы, как вы видите, сделаете их неконсистентными.  Есть варианты баз данных, где есть мастер-мастер-репликация, то есть репликация полная.

*00:28:07*
И в них нет как бы ведомой базы данных, и писать можно любую.  В этом случае они полностью синхронизированы друг с другом, и вам СУПД гарантирует то, что если в одну из баз данных попали, то они окажутся в остальных.  Бывают нюансы и бывают базы, которые делают это достаточно хитро, то есть там не обязательно, но разные политики есть, вот там, гарантии записи, особенно это в нереализованных базах данных характера.  В принципе, для реализованных баз данных стандартная история это мастер-свейт-репликация.  Мастер-мастер на SQL довольно сложно сделать.  Но можно, вроде бы. Я никогда не пробовал.  Мастер-мастер-репликация это нормальная история для нереализованных баз данных, таких как какой-нибудь ClickHouse, Cassandra и тому подобное.  Они работают чуть хитрее, они создают несколько реплик, каждый из которых выступает в роли мастера, и которые обменивают друг другу данные прямо в момент запроса.  И при этом вы можете выставить разные гарантии.  Например, ClickHouse там какой-нибудь или еще что-то, будет считать, что данные в FBD появились, когда они есть на большинстве реплик.  А потом, если у вас какое-то количество реплик отваливается, то, как он смотрит, на большинстве данных есть, значит так и надо, а если нет, ну ладно, жаль.  Есть нюанс, что если у вас кластер разваливается настолько, что у него меньше половины, например, типа, реплик остается, то он в целом разваливается, значит, я больше не работаю.  Но это уже совсем деталь.  Итак, мы с вами решили проблему того, что у нас, да, ведущая, ведомая АПД может в автоматическом режиме, Либо в ручном режиме АК, системный инженер поддержки или еще кто-нибудь увидел красную горящую кнопку на мониторинге, испугался, дернул рычаги и, короче, тут база стала ведущей вместо ведомой.  Либо это прямо автоматически происходит, когда одна база падает, ваш автоматик, понимая, что база упала, она стала недоступной, давайте мы какую-нибудь рандомную ведомую базу сделаем ведущей.  Опять же, в зависимости от системы, от того, какую СППД вы выбрали, традиционный, не традиционный, от конкретных нюансов, надстроек ваших на традиционной базе или не традиционной, у вас это будет происходить по-разному.  Важен сам факт того, что в случае отказа в ведущей ПД мы можем перекинуть нагрузку по записи на одну из ведомых, ну, соответственно, это стоит какое-то количество ресурсов, денег, еще чего-то, но, короче, времени скорее.

*00:30:46*
но он там может быть достаточно быстро произведен, плюс мы не потеряем колько из данных, можем продолжить работать. А еще мы с вами решили проблему того, что  теперь у нас читающие запросы отдельно идут от записывающих запросов, в общем-то их можно теперь больше, реплик можно сделать три, можно их, короче, ну, запросы как-то по этим репликам отбалансировать, да, вот, ну, ловко получается. Опять же в идеале субэдэ и клиенты для этой субэдэ, кстати, да, вот когда вам вы подключали какой-нибудь клиент для вашей базы данных, вы тоже используете на самом деле клиент серверной архитектуры, субэдэ выступает в роли сервера, ваше предложение в роли клиента.  В идеале клиент для вашей ВД, он получив конфигристом 4-5 строчек подключения базы данных, автоматически умеет ходить на любую из них.  Например, на чтение вы ему даете 3 банны для чтения, и он приходит.  Но не всегда это так работает, иногда это приходится обязательно.  Итак, с репликацией мы разобрались.  Вроде бы, теперь наша схема выглядит примерно вот так.  Ну, как на рисунке 1.6.  Окей?  Ну там, нарисована хотя бы одна ведомая BD, вроде так.  Чтение записи идут в них.  Эта схема понятна?  Эта схема понятна.  Да, по поводу перерыва.  Вы, пожалуйста, сами следите за временем, не тормозите меня, потому что я боюсь, что мы не успеем.  Все, я буду бежать. Если вам не нужно, то кричите в какой-то момент.  Следующая проблема, с которой мы сталкиваемся, очень большое количество одинаковых запросов, на которые мы выполняем полноценно другую операцию.  Ну, например, очень большое количество запросов на список товаров.  Или на карточку конкретного товара.  Или очень большое количество запросов на какую-то определенную страницу.  И так далее.  Эта проблема решается при помощи кэширования.  На самом деле, кэширование вы можете применить на разных уровнях, вы можете поставить как отдельный компонент вот в эту большую систему, прям отдельный кэш, такой как, например, Redis.  Это готовое программное решение, то есть готовая программа, которую вы просто запускаете, она, короче, работает на отдельной, отдельная программа на отдельной машине, может быть, а на нее можно отдельно выделить ресурсы, и вы просто к ней, опять же, по HTTP, входите, пишете в нее данные, забираете в нее данные, она работает супервыстро, ну, вот. Работает отдельно.  Соответственно, можете вставлять кэш внутрь всего приложения.  Ну, не знаю, там, какой-нибудь внутренний мастерчик просто себя организовывает, пишет ему данные, и все.

*00:33:31*
Вот. В этом случае вам придется самим часть, там, каких-то вещей организовать.  В чем суть кэша и почему это работает?  Кто-нибудь вам когда-нибудь рассказывал про то, что операция чтения данных с диска сильно дороже, чем операция чтения с оперативной памяти?  Когда-то это была абстрактная информация, да?  Вот теперь не совсем. Почему?  Чтение с базы данных занимает много времени.  И если у вашей системы тысячи запросов в секунду, в минуту, то каждый запрос, который пойдет читать что-то из базы данных, он, во-первых, нагрузку на базу данных генерирует, а во-вторых, он будет обрабатываться долго.  Если все эти запросы были про одну карточку замечательного товара, которую все решили прямо сейчас побежать купить, потому что его выкинули на рынок, нахера? Непонятно.  Зачем его каждый раз считать из базы?  Он же одинаковый.  Можно считать его из оперативной памяти.  И для этого и создан кэш.  Потому что чтение из оперативной памяти уже на порядке быстрее.  Соответственно, как это обычно работает?  Если веб-сервер получает запрос, идет считать в кэше, в кэше у него не получилось найти, тогда он идет считать без базы данных, записывает в кэше.  Если он нашел в кэше, то он просто возвращает данные в кэше.  Можно применять кэширование на разных уровнях.  Я сейчас, опять же, супербазу рассказал.  Много, к сожалению, вряд ли успею.  Три основных нюанса.  Во-первых, никогда не храните в кэше данные, которые, ну, вы не воспринимаете кэш, как точку хранения данных.  Для хранения есть база данных.  Кэш прям для хранения данных, которые вы хотите надежно хранить, которые вы не готовы терять.  Плохая такая.  Если у вас один кэш на все приложения, то это единая точка отказа.  Если этот кэш откажет, откажет все.  Так, как бы, ну, наверное, не надо делать.  И третье. Это срок хранения данных в кэше.  Не стоит делать данные в кэше бесконечно хранящимися.  Лучше раз в минуточку, раз в 10 минуточек, или еще во сколько-то выкинуть их из кэша.  Пусть один запросик или там 10 даже запросов параллельных сходят в базу, получат данные из базы и обнарядуют в кэше.  Это отлично вам поможет, когда кто-нибудь накробует, напишет какой-то не тот код, ошибочные данные залетят в кэш.  Ну, либо вам придется идти его руками чистить, либо у вас все само починится через 10 минут, когда у вас пакетик.  Понятно как работает?  Окей, про кэширование я тоже поговорил.  Следующая важная штука, про нее сейчас поговорю более отдельно, пожалуй, точнее более коротко.

*00:36:25*
У вас есть, у вашего приложения, какой-то набор файлов, которые вы возвращаете в неизменном виде.  Это готовые html-страницы, это JavaScript-код для клиента, это картинки, в общем, что-то, что просто файл.  У него не нужно ничего шаблонизировать, оно как бы готово и есть, и когда его запрашивают, вы его просто отдаёте.  Там как бы обычно BKM-сервер даже практически не задействован, обычно в этом случае просто балансировщик нагрузки, у него рядом, короче, маленький диск, на него приходит запрос куда-то туда, он такой, а, ну ладно.  С диска файлик просто отдаёт по сети, и всё, это очень быстро и кратко.  Ну, относительно быстро.  Обычно он там что-то фикширует, мне тоже не...  но если ваш замечательный клиент from united states of america делает запрос на ваш замечательный букет сервер который находится здесь в амстердаме то этот запрос полетит по проводу закопанному под водой и лететь он будет типа 100 миллисекунд просто попробуем При этом у нас есть на самом деле возможность сервер в United States в Америку поставить, ну или там в Китае или еще где-нибудь.  Короче, чем сервер ближе к вашему клиенту, тем быстрее проходит интернет-соединение и обмен сообщениями.  И для того, чтобы супербыстро отдавать вот такие вещи, как картинки, видеофайлы и так далее, существует такая штука как CDN, сеть доставки содержимого.  Как она работает? У вас есть исходный источник данных, он находится, я не знаю, в России, в Китае, в Европе где-то.  И ближе к вашим клиентам вы как бы арендуете у систем CDN, сервер, который автоматически настроен так, чтобы отдавать всю эту историю.  Там CDN записывается в...  Ну, обычно в этом случае, короче, адрес этих картин подменяется на адрес CDN'а, и CDN как-то настраивается так, чтобы он, типа, сначала... ну, опять же, он как кэш, короче, работает.  Сначала идёте к CDN'у.  Если в CDN'е нет, то CDN запросит из исходного источника.  Проблема в том, что CDN'ы это дорого, потому что вам за это нужно платить.  это отдельные сервера, они там типа как-то, ну, в общем, дорого.  А отдельные компании их предоставляют.  А во-вторых, вам важно предусмотреть, что будет, если CDN откажет.  Ну и прямо, типа вот, не работает.  Ваши приложения, по идее, в этом случае, должны сказать, ну-ка, нажать и пойти само в исходный источник данных, чтобы CDN не стал новой точкой от вас.  Какую проблему CDN решает, понятно?  Окей.  Что-то нарисовали.

*00:39:27*
кэш добавили, cdn добавили, вот мы с вами обсуждали-обсуждали и походу у нас появился набор проблем, которые мы решали через добавление кэша, cdn, балансировщика, репликации и т.д.  Какой вообще паттерн вы видите?  Разбитие на части?  Так, допустим.  Подходит, но я немножко продумаю.  Но в целом, да. Разбитие на части полезно.  То, что масштабируется горизонтально все?  Пока нет. Вообще нет.  Базы даток пока гризантально не масштабируются, например.  Да и сервера тоже так себе выглядят.  Да, но нет. Ну там типа есть нюансы.  Про масштабирование пока мы не сильно масштабировались.  Я там говорил, что масштабирование это кусочек проблем, но настоящих проблем с масштабированием у нас вообще нет.  Что я вам рассказывал в конце первого семинара?  Что из архитектуры надо реализовать только то, что требуется.  там еще был какой-то момент не только про архитектуру, а про общая система был какой-то общий паттерн, по которому мы добавляли новые элементы в эту схему Смотрите, в конце прошлого семинара я вам говорил, всегда решайте задачу, а не проблему.  Всегда ищите, сначала опишите проблему, а потом принимайте за решение.  Если вы сначала нашли решение, а потом пытаетесь подтянуть, что у него какая-то проблема есть, ну не надо.  Вот, смотрите, разработчик, придя куда-то, он может очень сильно захотеть разрабатывать микросервисное приложение.  Он может очень сильно захотеть попробовать новую классную модную технологию, которую он запишет себе в e-mail.  Так вот, ну, если вы решаете свою собственную проблему у своей собственной подкачки, может быть это нормальная тема.  А если вы решаете, на самом деле, проблему вашего заказчика, вашего клиента, там, я не знаю, ну, еще кого-то, вашего бизнеса, то вы должны, ну, типа, то в этом случае хороший инженер, даже разработчик инженер скорее, он не будет корячивать сюда технологию по принципу классное решение.  Он сначала посмотрит на проблему, которая возникает, потом ее сюда добавит.  А возможно, вы слышали, что микросервисы — это модно, стильно, молодежно.

*00:42:46*
это монолит, но он работает, и написать его возможно там сильно быстрее, возможно вы слышали про то, что там шардирование баз данных, какие-нибудь супер там современные классные базы данных, они насквозь тоже классные, но если у нас нету большой нагрузки, нафига нам это, можно использовать практически простое проверенное решение, короче есть на низком уровне программирование принцип и подсимпл список или его горами который я общая суть этих двух принципов заключается в следующем делай максимально просто из  возможного и не добавляй ничего лишнего в систему типа если это сейчас нужно системе да мы можем предусматривать точки расширения если мы понимаем что это точно произвел, что это решение точно потребуется, либо что это очень высокая вероятность, и нам нужно это предусмотреть. Если это есть, тогда в изначальном наборе архитектурных характеристик, которые нам нужны. Важный момент, я тут каждый раз, перед тем, как я что-то добавлял в эту схему, я вам говорил, но у нас есть проблема. Не хватает там трафика, может упасть база данных, данные слишком долго читаются, и так далее. Каждый из этих элементов решает какую-то проблему, у него появился И дальше будет точно то же самое.  Собственно, да.  Если у вас на текущий момент какие-то вопросы не хотелись, можно говорить.  Ну, можно будет еще задать вопросы.  Погнали дальше.  Начинаю от самой веселой и страшной штуки.  До этого было легко.  Хранение состояния.  Смотрите.  Помните, да, у нас там 2 сервера стоят рядом, и на них мы можем коммуницировать.  Вот, будет очень грустно, если эти сервера на самом деле хранят состояние, а запросы между ними, ну, как бы, балансируются рандомно.  Типа пользователь в какое-нибудь временное хранилище в кэше или там еще куда-то накидывает, я не знаю, корзину накидывает, корзина это пример какого-то состояния как раз пользователя это не совсем типа данные внутри базы данных, это скорее состояние конкретного пользователя или состояние конкретной операции пользователя, которую он собирается делать ну не самый характерный пример корзины, но вряд ли можно много придумать короче, пользователь накидывает, оно на сервере А в каком-то даже там кэше внутренней памяти сохранено а на сервере FBA ее нет.  Если пользователя запрос на просмотр казины, сначала он приходит на один сервер и там показывает казину, а на второй сервер приходит, казина не показывается, потому что на втором сервере, естественно, вполне ничего не сохранится.  Вот.

*00:45:08*
Это то, что может у вас произойти, если ваши сервера заняты состоянием.  Эту проблему можно решить при помощи всяких хитрых технологий, типа липких запросов, и далеко не всегда их легко решить.  Ну, во-первых, это, да, во-первых, нелегко, во-вторых, если у вас есть возможность, вот прям какая рекомендация, делайте сервера без состояния.  То есть делайте, чтобы этот сервер был просто штукой, которая умеет получить запрос, обработать этот запрос, и все.  За счет чего это можно сделать?  Это можно сделать за счет выноса хранилища состояния.  По факту это еще одна база данных, например, либо та же самая база данных, с которой вы работаете.  Либо, может быть, если мы готовы потерять состояние, важно, это может быть кэш.  Тот же самый кэш, либо другой кэш, аналогичное решение какое-то.  В этом случае любой из ваших запросов на любой из серверов сможет получить это состояние из кэша и обработать его и вернуть назад кэш либо это хранилище состояние часто в качестве вот этого разделяемого хранилища используют уже не революционную базу данных но могут использовать, потому что мы можем захотеть хранить какое-то состояние в достаточно призвольном формате его записывать в революционную базу данных довольно глутерно и не факт, что там нужно прямо революционность и это супер сохранение мы говорим просто стандартный паттерн выглядит примерно так берет запрос, читает состояние, получает состояние, обрабатывает, делает какие-то запросы, операции потом состояние сохраняет, потом возвращает пользу плюс там, конечно же, есть нюансы, но примерно так это обычно работает если есть возможность вынести это состояние, делайте это Потому что пока у вас вот так вот, вот так все устроено, да, вы либо вынуждены использовать какие-то липкие запросы, какие-то еще штуки, липкие запросы, которые от пользователя пришел серверу А, и дальше он будет ходить только к нему.  Это можно реализовать на бонусировочной нагрузке, но, ну, во-первых, не все отнесения, то есть коробки поддерживают, во-вторых, ну, это выплодные расходы, и если у вас сервер А отвалился, то что делать со всеми пользователями, которые как бы, ну, сервер А умер, типа диск у него умер, свет отключили, еще что-то произошло.  Все пользователи, которые ходили к этому серверу, родненькие, ну, жаль, наше состояние потерялось, что делать было понятно, вот.  А если у вас стейтлес, да, наиболее распространенное в IT обозначение для сервера, который не хранит состояние, это стейтлес сервер.

*00:47:45*
Либо стейтлес, соответственно, это если у вас сервер  с сохранением состояния.  Ну, алгоритм, программа, что угодно.  Возможно, было немножко сумбурно, но как будто бы достаточно.  И вот когда мы сделали с вами хранилище состояния отдельные, которые здесь нарисованы как no-scale, мы с вами можем начать легко решать проблему балансировки, точнее не балансировки, а нагрузки, автомасштабирования и, собственно, в принципе масштабирования.  Точнее масштабирование до этого мы с вами могли решать еще того что добавлять новые сервера но часто нагрузка она на самом деле не равномерно ну типа пользователи утром раньше ночью половина из них спит половина подрастают и я не знаю там вместо того чтобы спать серфит в айсберез на тему что-нибудь нового купить.  Днем, точнее утром, все пришли на работу и надо было первые пару часиков типа работать.  Потом мы уже работать устали, снова начинаю залипать в телефоне и трафик поднимается.  Трафик вашей системы может быть разным, зависит от времени, сезонности и еще каких-то условий.  Например, у нас, я напомню, мы занимаемся отправкой и рассылкой коммуникаций от бизнес-компаний до их клиентов, да?  Перед Черной Пятницей, перед Черной Пятницей, отправить можно просто конское безумное количество e-mails, sms-ов со словами приходите, пожалуйста, у нас тут карточки.  В общем, нагрузка в черную пятницу, типа, их два отмазывают просто.  Вот.  Ну и так может быть там, у разных систем в разное время.  Поэтому.  Классно, если вы можете под количество нагрузки масштабировать количество приложений.  Потому что, в общем-то, запущенное приложение, особенно если мы говорим про какие-то современные варианты их развертывания в облаках или еще где-то, Каждое запущенное приложение, оно кушает бабки.  Ну, оно кушает вычислительные мощности.  Если у вас есть система, что вы покупаете вычислительные мощности за бабки прямо сейчас, оно кушает ваши бабки.  Так делать не стоит, если оно просто простая.  Соответственно, можно, когда мы вынесли стейк, когда мы вынесли хранение, кошелек и все такое, и оно стоит у нас в сервисе, мы можем настроить какой-то автоматический балансировщик, который будет запускать приложение, когда-когда увеличивается объем нагрузки.  И выключать тогда, когда этой нагрузки не хватает.  Возможно, вы слышали такие волшебные слова, как «Докеру кубернетис».  Вот кубернетис — это такая огромная страшная штука, мы про нее говорить прямо сейчас не будем, но она, короче, так умеет.

*00:50:47*
Вы там можете, ну, типа, пару строчек написать, условно.  Пару строчек написать, а потом еще несколько месяцев не важно, почему она плохо работает, но, короче, реально в пару срочек можно сделать автоматическое масштабирование вот какого-то параметра.  Например, CPU usage, использование процессора, усоединенное между вот этими отдельными вашими элементами вот этих вот серверов, да, CPU usage стал больше, типа определённого порога, окей, пора добавлять новых инстанций, ну там, с каким-то ограничением максимальным.  Или потребление памяти стало больше, чем в какой-то паром, окей, добавляем еще сервер.  Вы можете настроить это, чтобы это работало автоматически, никакому инженеру не придется сидеть, мониторить какие-то графики, ничего нажимать, все будет работать само.  И не тратить в ночной период, там будет запущено 1-2 сервера, а в дневной период, когда загрузка большая, автоматически будет происходить скейлинг, так называемый, либо масштабирование, вот, загрузка.  Так, дальше к этому.  Мы с вами решили проблему, собственно, количества запросов в пиковые моменты и так далее.  А теперь мы начинаем решать куда более сложную, большую проблему.  Это защита от отказа.  Здесь появляется такое понятие, как SOD, в работе данных. То место, где у вас, если мы говорим про веб, где обычно размещаются сервера.  Ну, конечно, вы можете заместить их у себя где-то в компании, просто поставить в сервере радио, выкрутить для сервера и на них выпускаться на своем собственном железе.  Но чаще всего сейчас люди арендуют железо у какой-то компании, которая занимается организации всего этого. Вы, вероятно, слышали что-то про облако от Яндекс, облако от Mail.ru.  Это может быть облачное, а может быть железное.  То есть какой-нибудь SelectTail или какой-нибудь Dataline или еще какие-то компании, они могут вам предоставлять как виртуальные машины отдельные, на которых вы сами себе хозяин, и какой-нибудь готовый, развернутый версию кубернетиса в облаке, в которую вы просто дальше свое приложение накатываете и какие-то конфигурации управляете.  А железом управляет сама компания вот этого.  Все это можно условно назвать центр обработки, да, потому что все эти штуки размещены где-то там.  Так вот.  У нас есть несколько проблем.  Первая проблема, она связана все с тем же самым интернетом из США до Европы, который идет типа 100 миллисекунд, да.  И как бы мы, конечно, сделали CDN, но это вообще-то только для картинок.  Ну, типа там, запросы в базе данных он не ускоряет никак.

*00:53:46*
Поэтому, в принципе, мы с вами что можем сделать?  Мы можем развернуть полную копию наших приложений где-то в США, полную копию наших приложений где-то в Европе.  И у DNS'а, у него на самом деле сильно больше функций, И одна из этих функций, это он может выдавать разные IP-адреса, в зависимости от того, с какого IP-адреса приходит запрос.  Он примерно понимает геометку этого IP-адреса, вот, да, вас реально можно вычтить по IP, где вы, в какой стране вы находитесь.  Ну, с UPN, конечно, нет, но глобально, вот.  Соответственно, иногда там вплоть до города.  Вот от этого IP-адреса можно геомаршрутизировать в один центр обработки данных, либо в другой, соответственно, к одной версии приложения, либо к другой версии приложения.  Да, у них там свои IP-адреса и все такое.  Вторая проблема, которая на самом деле сильно более важная, ну, по крайней мере, на тех масштабах программ, с которыми я сталкивался, это отказы бывают разные.  Бывает, у вас диск умирает, и в среднем в каком-нибудь дата-центре Яндекса, у которого, типа, 500 или еще какого-то нагруженных дисков, диски умирают, типа, там, чаще, чем раз в день.  Это тупо рутинная операция, там, короче, инженеры такие, опять работа, идут, вынимают один диск, оставляют новый диск, вот.  А бывают отказы, когда, короче, ну, сейчас вот нейроничная история жизни, опять по-моему нашей компании, что-то вот подобного рода у нас было, где-то какие-то замечательные люди, то ли в городе, то ли не очень в городе, короче, решили построить то ли туалет, то ли какое-то здание. И в процессе этого строительства они ловко переругили кабель, который шел в дата-центр с интернетом. Ну и было смешно, да, но как говорю, все, что было в этом дата-центре, перестало быть доступным. Все. Все наши реплицированные базы данных, если они располагались в этом дата-центре, они как бы, ну, все они доступны.  Все приложения, которые там с маршрутизацией нагрузки, значит, есть, все они доступны.  Соответственно, как от такого можно защититься?  Ну, только разместить приложение в нескольких дата-центрах, чтобы перерубить то, что было в 3 провода, ну, и 2 провода.  На этой схеме есть проблема.  Как только вы добавляете несколько центров обработки данных, у вас появляются вот одни расклады, на самом деле, на то, чтобы синхронизировать между собой данные в BD.  Потому что вот то, что там база данных нарисована отдельно, они вы, на самом деле, должны бы также, ну, первую здесь как-то вынести, по-хорошему.

*00:56:32*
Почему? Ну, потому что если вы прям разделили отдельно базы данных CODA1 и CODA2,  то у них, получается, данные будут разные, и, типа, ну, это неправильная репликация получается.  Это не репликация, это уже разделение данных по, типа, отдельным CODA.  И если CODA1 становится недоступным, то все пользователи, которые имели данные в этом, ну, и все данные, которые были в этом CODA, тоже становятся недоступными.  а мы как бы не хотим, да?  Естественно, для этого нужно базы данных как-то носить отдельно.  Чаще всего что делают?  Базы данных либо размещают...  Ну, базы данных стараются, короче, разместить отдельно реплики, отдельно мастер-данные так, чтобы они размещались и там, и там.  То есть несколько их тоже.  И, ну, опять же, зависит от типа баз данных.  Но в идеале у вас, типа, часть...  по паре пары реплик лежит в стоде 1, пара реплик лежит в SODI2. Ну, кто-то из них мастер, понятно, но тут вам придется с этим делиться, если вы хотите полностью консистентный данный.  Вот, кэши можно держать в каждом SODI отдельно, потому что кэш, он временный, а чем локальнее и вместе в рамках одного SODI, обычно очень быстрый интернет, быстрая передача данных между сервисами, и там просто не десятки и сотни, а единицы миллисекунд проходят, поэтому кэш можно держать отдельно, когда, собственно, его основная функция быстро отвечать на запросы, будет работать быстрее, ну, из минусов, типа, если кэш не общий, то, ну, больше объема будет, потому что данные будут в одном кэше находиться, в другом кэше находиться, могут публиковаться.  Так, как примерно работает вот эта история с разными центрами обработки данных, понятно?  То есть это, грубо говоря, два больших дата-центра, мы в каждом из этих дата-центров, короче, свое приложение разворачиваем и настраиваем всю нашу систему так, чтобы в том числе балансировщик нагрузки, который в идеале, ну либо тоже виплицирован как между центрами, либо где-то отдельно в супернадежном месте стоит, но в целом если балансировщик умер, то увы и ах, все. К сожалению, это очень серьезная критичная точка отказа. Поэтому его вносит как отдельное приложение, часто используют готовые, написанные, проверенные и заставляют балансировщик заниматься только балансировкой, а не чем-нибудь еще.  Потому что если вы, балансировщик, начнете нагружать дополнительной нагрузкой, то шанс, что вы туда базу занесете, он, короче, увеличивается.  Вот. И балансировщик учат тоже так же, как и раньше, когда он между две сервисами балансировал нагрузку.

*00:59:02*
Его учат в желательном автоматическом режиме,  ну, либо там где-то инженеру, короче, красные лапочки выводят на Dashboard мониторинг, вот, что если тот один номер, то можно, короче, вернуть ушку и все-таки направить в 502.  Еще очень классно проводить учения.  Что произойдет, если 501 отключится?  Вот, мы так несколько раз делали.  И в целом, ну, как крупные взрослые компании так стараются делать.  Ну вот, мы так несколько раз делали и, в общем-то, поняли, что, как говорят, ну, репликация-то у нас есть.  И все это у нас как бы есть, только если работает.  Потом снили какое-то количество времени и улучшали всякие штуки, чтобы оно начало реально работать.  но по-любому все так следующая проблема смотрите у вас есть набор каких-то операций, приложений которые выполняются достаточно быстро а есть набор каких-то операций которые выполняются достаточно долго например, посчитать какой-нибудь отчет там, короче, нужно прям в базу залезть все данные из базы вытащить кучу математики применить, в общем, сложно, а главное, долго.  Может быть, это какая-то обработка картина.  Может быть, это необходимость отправить e-mail вашему получателю.  Ну, еще что-то. Короче, какая-то операция, у которой есть несколько характеристик.  Первое. Она может быть асинхронной, то есть не обязательно дать ответ прямо сейчас.  Можно, типа, сказать, мы положили в очередь, выполним, выполним. Вот.  Второе. Она, вероятность, не выводистая, Ну, не вас, я не имею в виду дополнительную подгруппу.  Вот.  Вот этих вот двух крипериях, ну вот, или хотя бы одного из них, достаточно для того, чтобы применить такую штуку, как очередь сообщения, либо бройкер сообщений.  В чем суть?  Вы создаете отдельное приложение, которое учится обрабатывать...  Ну, либо, на самом деле, это может быть не обязательно отдельное приложение, это может быть и отдельный процесс в рамках вашего же приложения, Но обычно все же стоит это сделать отдельным приложением, раз мы делаем такую историю с помощью этих сообщений.

*01:01:48*
В целом, вы можете очередь применить таким же способом и в рамках одного вашего приложения, но мы сейчас говорим про уровень более ушахи, когда развитие будет в разных положениях, и вот эти сообщения будут не просто объектом или структурой данных внутри вашего кода, а это прям отдельное, полноценное, так же как и кэш и база данных запускаемое приложение, написано каким-то сторонним разработчиком, который вы берете, запускаете, и оно умеет очень хорошо работать именно в формате очереди, то есть вы можете там внутри этого брокера сообщений создавать очереди какие-то, ну, что-то, что является  по факту очередью, писать туда какие-то данные, читать оттуда какие-то данные.  Две наиболее известные очереди сообщений, которые сейчас используются, брокера, который используется. Это RabbitMQ и Kafka.  В чем плюс? Теперь мы можем, когда нам на веб-сервис приходит запрос с генерированием отчета, мне говорит CSR, идти на 2 минуты генерировать отчет, а пользователи там прилово крутятся, короче, и все.  И в этот момент наш веб-сервер занял ресурс, и другие пользователи, которые отправляют запросы в этот же могут в какой-то момент начать получать ответ «сорян», либо их запросы будут тоже тупить, потому что этот леб-сервер свое СПУ, свои ресурсы загрузил в расчетам отчета для одного пользователя, а остальные способны продать.  Мы можем пользователю сказать «Окей, твой запрос на формирование отчета получит вот так.  Наткнись, и потом вот там посмотришь.  Тебе уведомление придет, или исчезнет».  Положить этот отчет в очередь и отдельным приложением, запущенным на отдельных выделенных узлах, на отдельных выделенных серверах, обрабатывать сообщение в этой очереди. То есть, когда сообщение туда приходит, какой-то рабочий узел берет, работает элемент работы и его обрабатывает. В общем-то, работает как такая стандартная очередь в, я не знаю, в кассах, в супермаркете, народ подходит, как-то распределяется по кассам и выполняет. Либо, скорее, у вас есть общая очередь на почте и несколько окошек выдачи в посылок. Ровно так это работает. Из плюсов. Какие проблемы мы решили?

*01:04:00*
Мы, на самом деле, решили проблему неравномерности вот этой загрузки между разными операциями. Потому что мы теперь можем отдельно масштабировать, уменьшать, увеличивать количество VM-узлов, отдельно уменьшать, увеличивать количество рабочих узлов. А еще у каждого из этих двух групп, у веб-сервисов, у рабочих сервисов, у рабочих узлов, ну либо в рабочих очередях, у них можно настроить свой профиль нагрузки, свое потребление ресурсов. Ну условно, для расчета отчетов нам  CPU не особо нужен, а вот памяти оперативной нужно у Google, потому что нужна память, вы говорите типа все. Или наоборот, нам нужно очень много CPU, но памяти нужно мало. Мы можем специально подстроить конфигурацию наших серверов или наших виртуальных машинок, на которых мы все будем запускать. Вот конкретные задачи. Это улучшает наше использование ресурсов и разгружает веб-серверы от, ну скажем так, делает более предсказуемо работающие. Из минусов.  Вообще сообщение от новой точки отказа, она в прошлом тоже должна быть если там критичные для вас данные или критичные операции.  И у вас начинаются приколы с тем, что вам нужно на уровень ниже когда-нибудь спустимся, там нужно уметь гарантировать доставку сообщений иногда, потому что нужно помнить, да, что в любой момент времени, нужно жить в парадигме, что в любой момент времени ваше приложение может, типа, ну, инфраструктура ненадежна, сеть ненадежна, все, что угодно ненадежно.  У вас может отключиться сеть, у вас может упасть приложение, у вас может что угодно случиться и в этот момент не должно произойти ситуации например что ваше сообщение не обработалось либо наоборот вы готовы такое терпеть и тогда ну дико другой поэтому говорить про это говорим подробно когда-нибудь потом мы поговорим так как работают очереди сообщений, понятно?  А можно вопрос?  Зачем вообще очередь?  Почему нельзя вот с сервера одного скатить в какую-то другую серверу блокировать?  Зачем это надо делать?  Да, но вам придется на этом своем сервере ожидать.  А, вот еще один момент, который в очереди сообщений разрешаю.  Ну, первый вопрос это если вам нужно гарантировать завершение операции. Вы типа попросили другой сервер, и он вам не скажет, да, окей, пока не выполнен расчет отчета, правильно? Это значит, что вы на своем сервере тоже будете ожидать, занимать поток, да, он будет в ожидании, в ожидании висеть, но висеть.  Количество потоков у вас будет расти. Да, это синхронно, да, еще что-то, но расти. Не очень круто.

*01:06:24*
Во-вторых, польские люди позволяют сгладить пиковые нагрузки. То есть, когда у вас какое-то  на русской фронте, генерируют запросы на кучу отчетов, а потом снова не генерируют.  Вот этот кик вы в очередь вложили, он там потом разберется стабильно, надежно и хранимо.  А если вы с новым сервисом просто синхронно ходите в другой сервис, ну, другой сервис обработчик, то если первый сервис, он получил запрос на 10 отчетов, он отправил эти 10 отчетов на рабочие узлы, а потом упал.  Что дальше будет? Непонятно.  Но вот этот набор проблем, короче, он не позволяет напрямую входить в другой рабочий узел.  То есть очередь она позволяет. Плюс вопрос балансировки сразу, да?  То есть очередь, она автоматически, на самом деле, там, умеет, вы можете, там, пять рабочих узлов создать, и они будут динамически вычитывать сообщения.  А как веб-сервис поймет о том, что рабочие узлы пора стелить, и что появился новый узел, к которому можно отправлять теперь тоже запросы?  То есть там сразу куча каких-то вопросов происходит.  Либо там какой-то дополнительный балансировщик нагрузки нужен между теми рабочими узлами.  Ну, короче, это вызывает дополнительные усложнения схемы.  А очередь имеет плюсом...  Она может быть персистентной и хранить сообщение даже после отключения электроэнергии или ее падения.  Она работает достаточно быстро и позволяет, собственно, положить в нее и забыть о существовании сообщения.  и продолжить обработку других штук это ключевой момент плюс сглаживание соответственно?  окей так, дальше идем у нас не так много времени осталось да, у нас там на самом деле крохозябра огромная получилась ой и всю эту крохозябру очень грустно делать если у вас нету инструментов для работы с ней а именно, если у вас нету настройного логирования, если у вас нет нормальных метрик вашего приложения, если у вас нет мониторинга и у вас нет автоматизации.  Про них, возможно, мы отдельно будем говорить.  Сейчас просто скажу, что такое логирование.  Вы, наверное, знаете, вы пишете в какой-нибудь файлик или там куда-нибудь, короче, какие-то сообщения о том, что происходит в вашем приложении.  Но это довольно слабая история, что, например, у вас много веб-сервисов.  Заходить на каждый и смотреть там файлик, а если, ну, короче, сложно.  Лучше, если ваши логи пишутся в какую-то отдельную систему, и в этой системе вы можете централизованно их просматривать.

*01:08:57*
Еще классная штука — структурированные логи, в которых есть отдельные поля, и вы практически как в отдельную базу данных, короче, пишите логи,  и потом можете по ним удобно искать.  Фильтруя по серверу, по запросу и так далее.  Метрики — тоже очень важная штука.  Метрики — это когда вы пишете в какую-то отдельную систему, для этого тоже есть готовые решения куча, такие как, например, Prometheus, Grafana и так далее.  Данные о каких-то бизнесовых метриках, либо и о бизнесовых, либо о каких-нибудь технических метриках.  Например, вы можете представить метрику CPU, загрузки CPU от CPU и загрузки по памяти ваших серверов, всех.  И иметь возможность мониторить и мочить с отказами, проблемами пользователей, когда вот вам пришли, вы открываете какой-то дашбот, на котором у вас вывезена эта метрика и смотрите, ага, у пользователей начались проблемы тогда-то, они начали приходить в саппорт тогда-то, а в этот момент загрузка нашего сервера была 100%.  Или еще более классная ситуация. У вас на метриках есть какие-то бизнесовые метрики, то есть вы там говорите время ответов пользователя, вы говорите, не знаю, количество обработанных каких-то сообщений, время обработки сообщений, еще что-то, время, не знаю, планирования отчета.  Если вы в какой-то момент видите по этим методам аномалии, вы можете в том числе настроить мониторинг на эти методы.  То есть вы можете настроить автоматические алерты, автоматические какие-то предупреждения вплоть до звонков дежурному разработчику или инженеру.  В случае, если я не знаю, цепу ваших серверов держится на 100% больше, чем полчаса.  Или время обработки сообщения продолжает расти, или размер какой-нибудь очереди сообщений монотонно растет.  Короче, вариантов куча.  У нас в компании это очень активно практикуется, в том числе и для того, чтобы гарантировать клиентам определенный уровень надежности нашей системы.  Мы гарантируем, что наше сообщение, например, определенного вида будет доставлено конечному получателю за не более чем за 3 минуты.  Для того, чтобы это гарантировать, мы, естественно, должны мы можем измерить время этих доставок или там, ну, еще выглядит количество измерений, или мы там гарантируем, что мы в час можем, по определенному типу клиентов, мы гарантируем отправку не менее, чем трех миллионов сообщений, именно, миллионы сообщений.  Соответственно, нужно измерять количество отправных сообщений, в случае, если мы отправляем меньше, чем на самом деле обещаем, мы должны на это, ну, среагировать автоматически.

*01:11:26*
Детали того, как это делать, я не рассказывал, просто рассказываю,  что так можно и стоит делать, и, скажем так, мы приступим к взаимной системе, так и стоит делать, и заранее об этом закладывается.  Автоматизация — это то, вы будете выкладывать ваш код на продакшн в так называемый CI-CD.  Про него, опять же, можете посчитать отдельный термин.  Очень полезный.  Так, у меня осталось соткнуть. Попробую успеть.  Собственно, мы, когда мы сталкиваемся с проблемой того, что у нас недостаточно ресурсов, ну, количество вопросов слишком большое, база данных начинает тупить и не перестает управляться.  Мы можем пойти двумя путями.  Можем масштабироваться вертикально, можем масштабироваться горизонтально.  Вертикальное масштабирование — это когда вы увеличиваете количество ресурсов на том же самом сервере.  Стоял сервер на 16 ядер и 32 гигабайта оперативной памяти.  Вы ставите туда новый процессор, новую память.  У вас получается сервер на 100 ядер и, не знаю, 0 гигабайт оперативной памяти.  У вертикального масштабирования есть две проблемы.  Первое, там есть конечный предел, до которого вы можете это все масштабировать.  Второе, это срост масштабирования.  Ценник растет нелинейно, он растет сильно больше.  и начинает с какого-то количества, там, абсолютно клонский ценник за какие-то супермощные сетапы.  Второй вариант — это горизонтальное масштабирование.  Это когда мы добавляем новые сервера и балансируем нагрузку между этими серверами.  В целом, в среднем, горизонтальное масштабирование обычно выдерживает у вертикального, но про вертикальное забывать тоже не стоит.  Горизонтальное круче, потому что у него сильно меньше предел масштабирования.  И сервера можно относительно дешево покупать.  Добрались до следующего понятия важного, которое называется шардирование.  Что такое шардирование?  Это когда у вас данные перестали вылезать на одну базу данных.  И логично в этом случае начать брать данные и писать их на несколько баз данных.  Например, как работает шардирование?  Вы придумываете какой-то критерий, по которому будете данные разделять.  Этот критерий называется ключом шардирования.  И по этому ключу, по этому какому-то ключу.  Самый простейший, это взять остаток отделения идентификатора за количество серверов.  Он плохой, так делать не стоит.

*01:14:18*
Возможно мы отдельно успеем поговорить про шардирование, если вам прям будет это интересно, конкретно эта тема, большому количеству людей, вы скажите, я там могу подстроиться под то, что вам звучит интересно и про что-то побольше рассказать.  Но глобальное шардирование, короче, сейчас вот так, концептуально, да, это мы перестали ввязываться в одну моду данных, данных стало слишком много, мы начинаем данные теперь распределять на несколько шардов и когда мы их читаем мы тоже должны понимать что мы должны прочитать соответствующих серверов по умолчанию SPL очень плохо подходит для того чтобы организовывать шардирование он не имеет встроенных готовых инструментов чтобы просто прочитать данные и их объединить в шардированный баз более современные новые SPL базы гораздо лучше с этим справляются.  Часто в них есть репликации шартирования из коробки.  Вам ничего с ними не нужно настраивать практически.  Вы просто разворачиваете нужное количество реплик, нужное количество шардов, и когда вы коннектируетесь к одному из них, они сами работают, они сами умеют отправить запросы, получив запрос на одну ноду, умеют отправить их на другие шарды, умеют на этих шардах получить данные, потом их как-то объединить на какой-то из нод и вернуть вам.  Примеры баз данных, которые умеют шардироваться и реплицироваться из коробки.  В чем минусы шардирования?  В том, что нам нужно как-то более хитро уметь джойнить эти запросы.  В том, что теперь нам нужно реплицировать каждый из шагов.  Раньше нам нужно было реплицировать одну базу, теперь нам нужно реплицировать каждый из шагов.  То есть у каждого шага на самом деле должно быть еще одна-три реплики минимум.  И тут мы уже получаем, что типа мы пошаргили на 4 хранилища и теперь у нас на самом деле уже минимум 12 баз данных.  Ну или ну окей, минимум 8, если реплика одна.  Соответственно ресурсов нам под это нужно побольше.  Мониторинга под все это нам нужно побольше.  Людей, которые в случае чего-то делают под это, нам тоже нужно побольше.  Но шаргирование решает проблему, которую по-другому никак не решишь.  Это объемно. В том числе, в какой-то момент емкость одного диска на базе просто заканчивается.  И нет дисков на тысячи, миллионы карабайтов, вот, к сожалению.  А хранитель придан иногда лучше. Поэтому шарпирование очень полезная штука.  Итого, мы с вами очень быстро, голодным проигроком, сегодня отошлись по Web-приложению, которое начиналось все с простого.

%*01:16:51*
Есть одно приложение на WCAN, есть одно приложение на конте, какое-то, ну, так, мобильное IEA.  И дошли до системы, в которой накручена репликация, шардирование, кэши, в очереди сообщений, выделены отдельные сервера, балансировка и куча-куча-куча других приколов.  Это нормальный путь развития веб-системы.  Это, ну, скажем так, это такой плюс-минус стандартный вариант архитектуры, кого, как устроен крупный сайт.  То есть, если вы придете в какой-нибудь там, на какой-нибудь доклад, там, не знаю, про архитектуру ВКонтакте или на какой-нибудь сайте ВК, Яндекс, ну вас вон еще куда-то начнете спрашивать скорее всего какие-то из этих ну большинство этих элементов они создадут где так понятное дело будут нюансы кто-то не шардируется подходом кто-то у кого-то монолиты нет очереди сообщений у кого-то сильно больше этих век сервера веб-сервисов там полноценная микросервисная архитектура может быть еще успеем у кого-то нет вынесенного хранилища состоянии, потому что им не нужно это крайнее состояние, у них и так, типа, все пользовательские сценарии стейлз, у них нету, типа, этого состояния. Но, в целом, вот эта штука, она неплохо описывает супер верхний уровень того, как эта штука работает.  Дальше у нас, да, время вопросов. Именно там еще два последних фокус-лайда. Про понятие микросервисной и моделительной архитектуры.  Я их, наверное, даже еще успею рассказать, но перед этим давайте я вот реально бежал и старался выместить полтора часа.  Можете задавать вопросы.  И в целом скажите, было понятно или непонятно.  Так, давайте.  Давайте начнем с того, было понятно или непонятно.  Вы тоже тут написал.  Понятно.  Окей.  Но у нас часть из этого была уже, да, в курсе по базе данных.

%*01:20:23*
ну у вас была репликация скорее всего и шага у вас частично и состояние работы конкретного пользователя например это может быть какая-нибудь штука или что-то другое что-то другое что-то другое у вас какой-нибудь визуальный конструктор чего-то, не знаю, сайта, не сайта, или какой-нибудь что еще может быть, ну да, допустим визуальный конструктор, и пользователь там набрасывает какой-то, ну, что-то, у него есть возможность там это, это что-то сохранить потом, да, базу данных, чтобы он сохранился, начал работать там или еще что-то. Но в процессе работы он, как бы, в вашем конструкции добавляет элементы, как-то их настраивает, и выходите, чтобы эта штука на самом деле у него, ну, тоже сохранялась как, ну, типа чернолитый, промежуточное состояние, да. Вот это вариант того, как бы, что может являться состояние. Казино пользователя в ком-нибудь интернет-магазине может являться  состоянием. То есть не уже совершенный заказ, а данный, ну а то что размещается в текущий указ. То есть еще не купленные товары. Что еще может быть? Какой-нибудь, я не знаю, браузерная игра, наверное, плохой пример, но тем не менее, вдруг какая-нибудь управленная игра или еще что-нибудь, вот состояние текущей игры под напользовательской типовой браузеркой может являться именно такой штукой, которую мы хотим применить. Ну, как, примерно, вот так.  То есть это, ну, на самом деле у нее там достаточно тонкая грань этого состояния и прям данных о пользователе в BD, и, ну, зачастую можно не выносить отдельно вот эту штуку, а хранить сразу на основной базе данных.  но иногда бывает что есть развесистое состояние или вот мы сейчас там начнем говорить про то что есть там сервисы у какой-то операции может быть отдельное свое состояние и как только у вас где-то появляется вот штука состоянием ее стоит вынести еще вопрос Давайте я пару слов скажу про монолитные микросервисы.  Ну, заметьте, на суперверховом уровне не так важно, что у нас там.  Монолитная, внутри веб-сервисов вот здесь.  Вот тут монолитная или микросервисная архитектура, какая она там внутри, какая это, работает, эту задачу выполняет, и все.  Но погружаясь чуть дуже, есть, собственно, два способа организации больших BKM-систем.  Первое.  Это монолит. Что это означает?  Это означает, что у вас есть одно запускаемое приложение, в рамках которого выполняются все функции.  Чем он хороший?  Такой подход.  У вас есть очень простое развертывание, вы запускаете одно приложение, и все работает.  Ну, если мы говорим, мы сейчас говорим, да, про...

%*01:22:27*
Вот, преимущественно, вот,  проработать эту штуку про все что внутри одного слота находится ну больше даже вот отдельный сервис он типа обеспечивает обработку всех запросов вот он простой в развертывании его поначалу просто разрабатывать нет накладных расходов на взаимодействие между сервисами то есть нам не нужно тратить время на запрос к любому сервису Мы, да, временно запрограммировали сервис.  Ожидать, ответ на этот сервис.  Все мы выполняем внутри нашего одного приложения.  И его легко запускать в дебажах на локальной машине.  Одно приложение запустили, это дебаж.  Но, когда у вас случается большой масштаб, когда у вас случается уже 50-100 разработчиков, эта репозитория начинает весить, GIT-репозитория начинает весить гигабайт.  И не потому, что вы туда загрузили бинарники или еще что-нибудь.  У нас есть такой репозиторий, если что, он реально весит больше, открывается и качается, он тоже стоит очень много.  И код от него написан на какие-то там десятки, сотни тысяч строчек, и писать к нему код больно.  Его сильно сложнее разрабатывать, его сложнее масштабировать, потому что у вас в этот огромный монолит уже потом, может быть, таки напитано много разного функционала, Каждую из этих функций вы хотите как-то отдельно помасштабировать по-хорошему, но не можете, потому что это одно приложение, и если масштабировать, то целиком все приложения.  Любое изменение — это перезапуск всего Monolith, то есть, когда у вас 100 разработчиков, каждый какое-то изменение приносит каждый день, вы каждый день будете вынуждены по огромному количеству раз перезапускать, перевыкладывать новые версии, выкладывать Monolith.  И в него будет сложно внедрять новую технологию, потому что он придется, типа, технологию внедрять на все большое приложение, скорее всего.  Но монолит — это неплохо. У него есть свои плюсы и свои минусы.  И часто монолита достаточно. И если его достаточно, это хорошо.  Есть альтернатива. Называется микросервисная архитектура.  Тут вот под «микро» — это прям человек, который это придумал, он потом сознавался, что я, говорит, плохое название придумал, типа, микро — это прям А.  Микро — оно не потому, что кода должно быть мало.  Микро — оно потому, что оно отвечает за небольшой участок домена и за небольшой участок пункта.  То есть оно микросервис.  Микро — маленькое.  Оно по задаче, не по объему кода.  Эту задачу может решать гигантский код.  Но задача простая от Амара, отдельная.

%*01:25:13*
Что мы делаем? Мы выделяем набор отдельных приложений, настраиваем взаимодействие между ними посредством сети,  передачи данных через очереди сообщений, еще как.  И у каждого микросервиса, которому нужно хранение данных, мы делаем свое хранение данных. Почему?  Потому что если мы всех заставим ходить в одну и ту же базу данных, то как бы мы... А, ну да, давайте начнем с плюсов.  Мы имеем раздельный диплой, то есть раздельное развертывание этих приложений.  Мы имеем возможность как бы отделить изменения от одной десятки разработчиков, от другой десятки разработчиков, то есть от 100 остальных разработчиков, и запускать эти изменения более-менее отдельно, изолированно.  Мы имеем возможность проще масштабировать каждую из этих отдельных сервисов.  Там, где нагрузки сильно много, там мы увеличиваем объем.  Ну, реплик, то, что мы раньше говорили, да, можем автоматически его изменять, если запускаем сейчас соответствующие средства.  Мы можем гарантировать пользовательческую доступность.  Команда плохих разработчиков в своей какой-то отдельной фиче накрабовала и сделала так, что их сервис выжирает всю память и падает с out of memory.  Вот случилось с Monolith'ом, он выжрет всю память в Monolith'е и крашится в Monolith'е.  и все остальные пункции перестанут работать.  С лучшим микросервисом крашится конкретный микросервис этих плохих ребят.  И если он не был отвечающим за самую критичную функциональную систему, типа авторизации или еще чего-нибудь, что нужно везде, либо большая часть функционалов продолжит работать, и, может быть, половина пользователей вообще не заметит, что у нас что-то сломалось.  Так что мы более откажустойчивы, в целом, к микросервису.  И на большом масштабе маленькие сервисы проще разрабатывать.  Но потому что в среднем они действительно все же становятся меньше по объему кода.  Но, опять же, в первую очередь они должны быть меньше по объему ответственности.  И вот эта ответственность, она на самом деле чуть ли не больше значит.  Потому что, когда у вас есть огромный-огромный-огромный-огромный монолит, то у вас контекстов у разработчиков в голове должно быть очень много.  У него когнитивная нагрузка при разработке вот этого огромного монолита очень большая.  Потому что он вот сейчас сюда что-нибудь напишет, то он должен в голове типа смоделировать, А что еще я могу сломательно зацепить вот этим своим изменением в этой вот огромной крокодиле?

%*01:27:33*
Когда у него микросервис, в котором он знает, что вот этот сервис отвечает, он, типа, принимает из очереди сообщение e-mail и его отправляет.  Ну, там, не надо держать сильно много контекста, нужно понимать, как отправлять e-mail.  Сильно проще разрабатывать.  Но в чем проблема?  Было вообще легко и изящно взять и развернуть один монолит.  Теперь мы написали 10 микросервисов.  И теперь у нас не получится легко и изящно развернуть 10 микросервисов.  Нам придется просто поболбаться с этим.  Нам придется детальнее изучить кубернетис, или докер, или какую-нибудь еще штуку.  Для локальной оплатки нам придется какие-то отдельные конфигурации делать, или отдельный стенджинг к систему делать, или еще что-то.  Нам придется весело и задорно синхронизировать данные между этими сервисами.  Почему? Потому что отдельная АПД не очень хорошая идея, потому что вот раньше в монолите опять какая-то команда написала страшный запрос базу приложила всю базу монолита все остальные системы страдают и нам разделили все микросервисы мы иного входим в одну и ту же базу у нас остается единая точка в которую получается все разработчики всех микросервисов будут писать изменения а это тут же нивелирую часть плюсов которые у нас были лучше чтобы у каждого микросервиса была своя база данных и сервис и ПД были изолированы от остальных, насколько это возможно. Там договоренности были на уровне контрактов между этими сервисами, то есть там протокол передач, который вы выбрали, и они менялись в меньшей степени. А вот все, что внутри, можно было изменять достаточно свободно. И тогда вы получите максимальный буст, собственно, к скорости разработки, к раздельности деплоя и к отказу устойчивости. Потому что то, как вы Если вы повлияете на базу своего микросервиса, действительно в этом случае затронет только вас.  Если у вас все микросервисы входят в эту базу, то один микросервис может положить всех остальных.  А еще миграции для всех микросервисов в эту базу нужно будет прокатывать, а миграции на большую базу не всегда.  Быстрый процесс.  В общем, микросервисы — это круто, но не всегда.  Они создают много накладных расходов на различные штуки.  И это всегда нужно помнить, когда вы собираетесь делать микросервис.  Нормальный паттерн начинается с Monolith.  Закладываем в него возможность, потом отделить какие-то куски, наделим сервис, и потом постепенно выполняем.  Писать сразу микросервисы можно, но нужно хорошо понимать, на что вы идете, какие передов у вас будут.

%*01:30:10*
Приработив с микросервисом, для вас критично, прям, знание DevOps каких-то штук, для вас критично понимание, как настраивать,  сильно больше важно умение настраивать балансировщики, кубернетисы и прочие штуки, сильно критично становится мониторинг, и для того, чтобы понимать, что у вас работает хорошо, а что плохо.  Возможно, мы с вами об этом еще подробнее поговорим.  На этом я, я так уважил вас, забежал.

	
\end{document}
